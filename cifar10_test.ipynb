{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShuuTsubaki/fitting-random-labels/blob/master/cifar10_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5gHi0luSdqC",
        "colab_type": "code",
        "outputId": "1aa413cb-e2fe-4188-9473-b14ad00882e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPOnwqYMSrKZ",
        "colab_type": "code",
        "outputId": "470ec307-a239-4863-fe1a-242f799d492f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!ls /content/gdrive/My\\ Drive/Spring2019/CS\\ 4284/random-labels-given-other-neutral-network\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar100_data.py  data\t\tmodel_wideresnet.py  runs\t    train.py\n",
            "cifar10_data.py   LICENSE\t__pycache__\t     stl10_data.py\n",
            "cmd_args.py\t  model_mlp.py\tREADME.md\t     test.py\n",
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRgX3wxlSzIt",
        "colab_type": "code",
        "outputId": "360668a0-3609-4726-fd96-24493e60c48e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6846
        }
      },
      "source": [
        "!cp -r /content/gdrive/My\\ Drive/Spring2019/CS\\ 4284/random-labels-given-other-neutral-network/* ./\n",
        "!cat train.py\n",
        "!cat stl10_data.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from __future__ import print_function\n",
            "\n",
            "import os\n",
            "import logging\n",
            "import numpy as np\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torchvision.transforms as transforms\n",
            "import torch.backends.cudnn as cudnn\n",
            "import torch.optim\n",
            "\n",
            "from cifar10_data import CIFAR10RandomLabels\n",
            "from cifar100_data import CIFAR100RandomLabels\n",
            "from stl10_data import STL10RandomLabels\n",
            "import cmd_args\n",
            "import model_mlp, model_wideresnet\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def get_data_loaders(args, shuffle_train=True):\n",
            "  if args.data == 'cifar10':\n",
            "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
            "                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
            "\n",
            "    if args.data_augmentation:\n",
            "      transform_train = transforms.Compose([\n",
            "          transforms.RandomCrop(32, padding=4),\n",
            "          transforms.RandomHorizontalFlip(),\n",
            "          transforms.ToTensor(),\n",
            "          normalize,\n",
            "          ])\n",
            "    else:\n",
            "      transform_train = transforms.Compose([\n",
            "          transforms.ToTensor(),\n",
            "          normalize,\n",
            "          ])\n",
            "\n",
            "    transform_test = transforms.Compose([\n",
            "        transforms.ToTensor(),\n",
            "        normalize\n",
            "        ])\n",
            "\n",
            "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
            "    train_loader = torch.utils.data.DataLoader(\n",
            "        CIFAR10RandomLabels(root='./data', train=True, download=True,\n",
            "                            transform=transform_train, num_classes=args.num_classes,\n",
            "                            corrupt_prob=args.label_corrupt_prob),\n",
            "        batch_size=args.batch_size, shuffle=shuffle_train, **kwargs)\n",
            "    val_loader = torch.utils.data.DataLoader(\n",
            "        CIFAR10RandomLabels(root='./data', train=False,\n",
            "                            transform=transform_test, num_classes=args.num_classes,\n",
            "                            corrupt_prob=args.label_corrupt_prob),\n",
            "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
            "\n",
            "    return train_loader, val_loader\n",
            "  elif args.data == 'cifar100':\n",
            "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
            "                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
            "\n",
            "    if args.data_augmentation:\n",
            "      transform_train = transforms.Compose([\n",
            "          transforms.RandomCrop(32, padding=4),\n",
            "          transforms.RandomHorizontalFlip(),\n",
            "          transforms.ToTensor(),\n",
            "          normalize,\n",
            "          ])\n",
            "    else:\n",
            "      transform_train = transforms.Compose([\n",
            "          transforms.ToTensor(),\n",
            "          normalize,\n",
            "          ])\n",
            "\n",
            "    transform_test = transforms.Compose([\n",
            "        transforms.ToTensor(),\n",
            "        normalize\n",
            "        ])\n",
            "\n",
            "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
            "    train_loader = torch.utils.data.DataLoader(\n",
            "        CIFAR100RandomLabels(root='./CIFAR100data', train=True, download=True,\n",
            "                            transform=transform_train, num_classes=args.num_classes,\n",
            "                            corrupt_prob=args.label_corrupt_prob),\n",
            "        batch_size=args.batch_size, shuffle=shuffle_train, **kwargs)\n",
            "    val_loader = torch.utils.data.DataLoader(\n",
            "        CIFAR100RandomLabels(root='./CIFAR100data', train=False,\n",
            "                            transform=transform_test, num_classes=args.num_classes,\n",
            "                            corrupt_prob=args.label_corrupt_prob),\n",
            "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
            "\n",
            "    return train_loader, val_loader\n",
            "  elif args.data == 'stl10':\n",
            "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
            "                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
            "\n",
            "    if args.data_augmentation:\n",
            "      transform_train = transforms.Compose([\n",
            "          transforms.RandomCrop(32, padding=4),\n",
            "          transforms.RandomHorizontalFlip(),\n",
            "          transforms.ToTensor(),\n",
            "          normalize,\n",
            "          ])\n",
            "    else:\n",
            "      transform_train = transforms.Compose([\n",
            "          transforms.ToTensor(),\n",
            "          normalize,\n",
            "          ])\n",
            "\n",
            "    transform_test = transforms.Compose([\n",
            "        transforms.ToTensor(),\n",
            "        normalize\n",
            "        ])\n",
            "\n",
            "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
            "    train_loader = torch.utils.data.DataLoader(\n",
            "        STL10RandomLabels(root='./STL100data', split='train', download=True,\n",
            "                            transform=transform_train, num_classes=args.num_classes,\n",
            "                            corrupt_prob=args.label_corrupt_prob),\n",
            "        batch_size=args.batch_size, shuffle=shuffle_train, **kwargs)\n",
            "    val_loader = torch.utils.data.DataLoader(\n",
            "        STL10RandomLabels(root='./STL100data', split='test',\n",
            "                            transform=transform_test, num_classes=args.num_classes,\n",
            "                            corrupt_prob=args.label_corrupt_prob),\n",
            "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
            "\n",
            "    return train_loader, val_loader\n",
            "  else:\n",
            "    raise Exception('Unsupported dataset: {0}'.format(args.data))\n",
            "\n",
            "\n",
            "def get_model(args):\n",
            "  # create model\n",
            "  if args.arch == 'wide-resnet':\n",
            "    model = model_wideresnet.WideResNet(args.wrn_depth, args.num_classes,\n",
            "                                        args.wrn_widen_factor,\n",
            "                                        drop_rate=args.wrn_droprate)\n",
            "  elif args.arch == 'mlp':\n",
            "    n_units = [int(x) for x in args.mlp_spec.split('x')] # hidden dims\n",
            "    n_units.append(args.num_classes)  # output dim\n",
            "    n_units.insert(0, 32*32*3)        # input dim\n",
            "    model = model_mlp.MLP(n_units)\n",
            "\n",
            "  # for training on multiple GPUs.\n",
            "  # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
            "  # model = torch.nn.DataParallel(model).cuda()\n",
            "  model = model.cuda()\n",
            "\n",
            "  return model\n",
            "\n",
            "\n",
            "def train_model(args, model, train_loader, val_loader,\n",
            "                start_epoch=None, epochs=None):\n",
            "  cudnn.benchmark = True\n",
            "\n",
            "  # define loss function (criterion) and pptimizer\n",
            "  criterion = nn.CrossEntropyLoss().cuda()\n",
            "  optimizer = torch.optim.SGD(model.parameters(), args.learning_rate,\n",
            "                              momentum=args.momentum,\n",
            "                              weight_decay=args.weight_decay)\n",
            "\n",
            "  start_epoch = start_epoch or 0\n",
            "  epochs = epochs or args.epochs\n",
            "\n",
            "  # add plot data\n",
            "  train_accuracy = []\n",
            "  test_accuracy = []\n",
            "  train_loss = []\n",
            "  test_loss = []\n",
            "\n",
            "  for epoch in range(start_epoch, epochs):\n",
            "    adjust_learning_rate(optimizer, epoch, args)\n",
            "\n",
            "    # train for one epoch\n",
            "    tr_loss, tr_prec1 = train_epoch(train_loader, model, criterion, optimizer, epoch, args)\n",
            "\n",
            "    # evaluate on validation set\n",
            "    val_loss, val_prec1 = validate_epoch(val_loader, model, criterion, epoch, args)\n",
            "\n",
            "    if args.eval_full_trainset:\n",
            "      tr_loss, tr_prec1 = validate_epoch(train_loader, model, criterion, epoch, args)\n",
            "\n",
            "    logging.info('%03d: Acc-tr: %6.2f, Acc-val: %6.2f, L-tr: %6.4f, L-val: %6.4f',\n",
            "                 epoch, tr_prec1, val_prec1, tr_loss, val_loss)\n",
            "    test_accuracy.append(val_prec1)\n",
            "    test_loss.append(val_loss)\n",
            "    train_loss.append(tr_loss)\n",
            "    train_accuracy.append(tr_prec1)\n",
            "\n",
            "  # Plot results of perceptron training\n",
            "\n",
            "\n",
            "\n",
            "  plt.subplot(121)\n",
            "  train_line = plt.plot(range(start_epoch, epochs), train_accuracy, label=\"Training\")\n",
            "  test_line = plt.plot(range(start_epoch, epochs), test_accuracy, label=\"Testing\")\n",
            "  plt.title('Dataset')\n",
            "  plt.xlabel('Iteration')\n",
            "  plt.ylabel('Accuracy')\n",
            "  plt.legend()\n",
            "\n",
            "\n",
            "\n",
            "  plt.subplot(122)\n",
            "  train_line = plt.plot(range(start_epoch, epochs), train_loss, label=\"Training\")\n",
            "  test_line = plt.plot(range(start_epoch, epochs), test_loss, label=\"Testing\")\n",
            "  plt.title('Dataset')\n",
            "  plt.xlabel('Iteration')\n",
            "  plt.ylabel('Accuracy')\n",
            "  plt.legend()\n",
            "  plt.show()\n",
            "  return test_accuracy, test_loss, train_accuracy, train_loss\n",
            "\n",
            "def train_epoch(train_loader, model, criterion, optimizer, epoch, args):\n",
            "  \"\"\"Train for one epoch on the training set\"\"\"\n",
            "  batch_time = AverageMeter()\n",
            "  losses = AverageMeter()\n",
            "  top1 = AverageMeter()\n",
            "\n",
            "  # switch to train mode\n",
            "  model.train()\n",
            "\n",
            "  for i, (input, target) in enumerate(train_loader):\n",
            "    # target = target.cuda(async=True)\n",
            "    target = target.cuda()\n",
            "    input = input.cuda()\n",
            "    input_var = torch.autograd.Variable(input)\n",
            "    target_var = torch.autograd.Variable(target)\n",
            "\n",
            "    # compute output\n",
            "    output = model(input_var)\n",
            "    loss = criterion(output, target_var)\n",
            "\n",
            "    # measure accuracy and record loss\n",
            "    prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
            "    # losses.update(loss.data[0], input.size(0))\n",
            "    losses.update(loss.data, input.size(0))\n",
            "    # top1.update(prec1[0], input.size(0))\n",
            "    top1.update(prec1, input.size(0))\n",
            "    # compute gradient and do SGD step\n",
            "    optimizer.zero_grad()\n",
            "    loss.backward()\n",
            "    optimizer.step()\n",
            "\n",
            "  return losses.avg, top1.avg\n",
            "\n",
            "\n",
            "def validate_epoch(val_loader, model, criterion, epoch, args):\n",
            "  \"\"\"Perform validation on the validation set\"\"\"\n",
            "  batch_time = AverageMeter()\n",
            "  losses = AverageMeter()\n",
            "  top1 = AverageMeter()\n",
            "\n",
            "  # switch to evaluate mode\n",
            "  model.eval()\n",
            "\n",
            "  for i, (input, target) in enumerate(val_loader):\n",
            "    # target = target.cuda(async=True)\n",
            "    target = target.cuda()\n",
            "    input = input.cuda()\n",
            "    input_var = torch.autograd.Variable(input, volatile=True)\n",
            "    target_var = torch.autograd.Variable(target, volatile=True)\n",
            "\n",
            "    # compute output\n",
            "    output = model(input_var)\n",
            "    loss = criterion(output, target_var)\n",
            "\n",
            "    # measure accuracy and record loss\n",
            "    prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
            "    # losses.update(loss.data[0], input.size(0))\n",
            "    # top1.update(prec1[0], input.size(0))\n",
            "    losses.update(loss.data, input.size(0))\n",
            "    top1.update(prec1, input.size(0))\n",
            "  return losses.avg, top1.avg\n",
            "\n",
            "\n",
            "class AverageMeter(object):\n",
            "  \"\"\"Computes and stores the average and current value\"\"\"\n",
            "  def __init__(self):\n",
            "    self.reset()\n",
            "\n",
            "  def reset(self):\n",
            "    self.val = 0\n",
            "    self.avg = 0\n",
            "    self.sum = 0\n",
            "    self.count = 0\n",
            "\n",
            "  def update(self, val, n=1):\n",
            "    self.val = val\n",
            "    self.sum += val * n\n",
            "    self.count += n\n",
            "    self.avg = self.sum / self.count\n",
            "\n",
            "\n",
            "def adjust_learning_rate(optimizer, epoch, args):\n",
            "  \"\"\"Sets the learning rate to the initial LR decayed by 10 after 150 and 225 epochs\"\"\"\n",
            "  lr = args.learning_rate * (0.1 ** (epoch // 150)) * (0.1 ** (epoch // 225))\n",
            "  for param_group in optimizer.param_groups:\n",
            "      param_group['lr'] = lr\n",
            "\n",
            "\n",
            "def accuracy(output, target, topk=(1,)):\n",
            "  \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
            "  maxk = max(topk)\n",
            "  batch_size = target.size(0)\n",
            "\n",
            "  _, pred = output.topk(maxk, 1, True, True)\n",
            "  pred = pred.t()\n",
            "  correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
            "\n",
            "  res = []\n",
            "  for k in topk:\n",
            "      correct_k = correct[:k].view(-1).float().sum(0)\n",
            "      res.append(correct_k.mul_(100.0 / batch_size))\n",
            "  return res\n",
            "\n",
            "\n",
            "def setup_logging(args):\n",
            "  import datetime\n",
            "  exp_dir = os.path.join('runs', args.exp_name)\n",
            "  if not os.path.isdir(exp_dir):\n",
            "    os.makedirs(exp_dir)\n",
            "  log_fn = os.path.join(exp_dir, \"LOG.{0}.txt\".format(datetime.date.today().strftime(\"%y%m%d\")))\n",
            "  logging.basicConfig(filename=log_fn, filemode='w', level=logging.DEBUG)\n",
            "  # also log into console\n",
            "  console = logging.StreamHandler()\n",
            "  console.setLevel(logging.INFO)\n",
            "  logging.getLogger('').addHandler(console)\n",
            "  print('Logging into %s...' % exp_dir)\n",
            "\n",
            "\n",
            "def main():\n",
            "  args = cmd_args.parse_args()\n",
            "  print(args)\n",
            "  setup_logging(args)\n",
            "\n",
            "  if args.command == 'train':\n",
            "    train_loader, val_loader = get_data_loaders(args, shuffle_train=True)\n",
            "    model = get_model(args)\n",
            "    logging.info('Number of parameters: %d', sum([p.data.nelement() for p in model.parameters()]))\n",
            "    test_accuracy, test_loss, train_accuracy, train_loss = train_model(args, model, train_loader, val_loader)\n",
            "  return test_accuracy, test_loss, train_accuracy, train_loss\n",
            "\n",
            "if __name__ == '__main__':\n",
            "  main()\n",
            "\n",
            "\"\"\"\n",
            "stl-10 dataset, with support for random labels\n",
            "\"\"\"\n",
            "import numpy as np\n",
            "\n",
            "import torch\n",
            "import torchvision.datasets as datasets\n",
            "\n",
            "\n",
            "class STL10RandomLabels(datasets.STL10):\n",
            "  \"\"\"STL10 dataset, with support for randomly corrupt labels.\n",
            "\n",
            "  Params\n",
            "  ------\n",
            "  corrupt_prob: float\n",
            "    Default 0.0. The probability of a label being replaced with\n",
            "    random label.\n",
            "  num_classes: int\n",
            "    Default 10. The number of classes in the dataset.\n",
            "  \"\"\"\n",
            "  def __init__(self, corrupt_prob=0.0, num_classes=10, **kwargs):\n",
            "    super(STL10RandomLabels, self).__init__(**kwargs)\n",
            "    self.n_classes = num_classes\n",
            "    if self.split == 'train':\n",
            "      self.train_labels = self.labels\n",
            "    else:\n",
            "      self.test_labels = self.labels\n",
            "    if corrupt_prob > 0:\n",
            "      self.corrupt_labels(corrupt_prob)\n",
            "\n",
            "  def corrupt_labels(self, corrupt_prob):\n",
            "    labels = np.array(self.train_labels if self.split == 'train' else self.test_labels)\n",
            "    np.random.seed(12345)\n",
            "    mask = np.random.rand(len(labels)) <= corrupt_prob\n",
            "    rnd_labels = np.random.choice(self.n_classes, mask.sum())\n",
            "    labels[mask] = rnd_labels\n",
            "    # we need to explicitly cast the labels from npy.int64 to\n",
            "    # builtin int type, otherwise pytorch will fail...\n",
            "    labels = [int(x) for x in labels]\n",
            "\n",
            "    if self.split == 'train':\n",
            "      self.train_labels = labels\n",
            "    else:\n",
            "      self.test_labels = labels"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWVx4getS1EU",
        "colab_type": "code",
        "outputId": "98a3e95f-0bb3-4803-9bb4-1b01e9cab9c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5475
        }
      },
      "source": [
        "!python train.py --label-corrupt-prob=1.0 "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(arch='wide-resnet', batch_size=128, command='train', data='cifar10', data_augmentation=False, epochs=300, eval_full_trainset=True, exp_name='cifar10_corrupt1_wide-resnet28-1_lr0.1_mmt0.9_Wd0.0001_NoAug', label_corrupt_prob=1.0, learning_rate=0.1, mlp_spec='512', momentum=0.9, name='', num_classes=10, weight_decay=0.0001, wrn_depth=28, wrn_droprate=0.0, wrn_widen_factor=1)\n",
            "Logging into runs/cifar10_corrupt1_wide-resnet28-1_lr0.1_mmt0.9_Wd0.0001_NoAug...\n",
            "Files already downloaded and verified\n",
            "Number of parameters: 369498\n",
            "train.py:259: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input_var = torch.autograd.Variable(input, volatile=True)\n",
            "train.py:260: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  target_var = torch.autograd.Variable(target, volatile=True)\n",
            "000: Acc-tr:  59.89, Acc-val:  59.01, L-tr: 1.1491, L-val: 1.1794\n",
            "001: Acc-tr:  70.86, Acc-val:  68.70, L-tr: 0.8266, L-val: 0.8940\n",
            "002: Acc-tr:  73.00, Acc-val:  71.16, L-tr: 0.7782, L-val: 0.8674\n",
            "003: Acc-tr:  78.26, Acc-val:  74.35, L-tr: 0.6163, L-val: 0.7423\n",
            "004: Acc-tr:  80.07, Acc-val:  75.50, L-tr: 0.5830, L-val: 0.7591\n",
            "005: Acc-tr:  83.93, Acc-val:  78.48, L-tr: 0.4606, L-val: 0.6633\n",
            "006: Acc-tr:  84.94, Acc-val:  79.01, L-tr: 0.4244, L-val: 0.6358\n",
            "007: Acc-tr:  89.11, Acc-val:  81.88, L-tr: 0.3120, L-val: 0.5663\n",
            "008: Acc-tr:  88.03, Acc-val:  79.46, L-tr: 0.3442, L-val: 0.6330\n",
            "009: Acc-tr:  90.17, Acc-val:  81.06, L-tr: 0.2866, L-val: 0.5930\n",
            "010: Acc-tr:  90.33, Acc-val:  80.51, L-tr: 0.2719, L-val: 0.6175\n",
            "011: Acc-tr:  91.24, Acc-val:  81.18, L-tr: 0.2510, L-val: 0.5944\n",
            "012: Acc-tr:  91.39, Acc-val:  80.72, L-tr: 0.2413, L-val: 0.6315\n",
            "013: Acc-tr:  93.87, Acc-val:  82.07, L-tr: 0.1763, L-val: 0.5785\n",
            "014: Acc-tr:  92.96, Acc-val:  81.13, L-tr: 0.2060, L-val: 0.5922\n",
            "015: Acc-tr:  87.25, Acc-val:  76.87, L-tr: 0.3747, L-val: 0.8439\n",
            "016: Acc-tr:  90.82, Acc-val:  79.80, L-tr: 0.2527, L-val: 0.7150\n",
            "017: Acc-tr:  89.74, Acc-val:  79.07, L-tr: 0.2857, L-val: 0.7847\n",
            "018: Acc-tr:  91.29, Acc-val:  79.28, L-tr: 0.2442, L-val: 0.7535\n",
            "019: Acc-tr:  91.52, Acc-val:  79.29, L-tr: 0.2351, L-val: 0.7087\n",
            "020: Acc-tr:  94.47, Acc-val:  81.48, L-tr: 0.1562, L-val: 0.6846\n",
            "021: Acc-tr:  92.50, Acc-val:  79.97, L-tr: 0.2153, L-val: 0.7784\n",
            "022: Acc-tr:  94.21, Acc-val:  80.23, L-tr: 0.1676, L-val: 0.7369\n",
            "023: Acc-tr:  96.20, Acc-val:  82.65, L-tr: 0.1083, L-val: 0.6436\n",
            "024: Acc-tr:  94.40, Acc-val:  80.76, L-tr: 0.1599, L-val: 0.7360\n",
            "025: Acc-tr:  95.49, Acc-val:  81.96, L-tr: 0.1268, L-val: 0.6503\n",
            "026: Acc-tr:  94.11, Acc-val:  80.85, L-tr: 0.1713, L-val: 0.7308\n",
            "027: Acc-tr:  96.42, Acc-val:  82.37, L-tr: 0.1026, L-val: 0.6403\n",
            "028: Acc-tr:  92.13, Acc-val:  79.59, L-tr: 0.2263, L-val: 0.8373\n",
            "029: Acc-tr:  92.09, Acc-val:  78.95, L-tr: 0.2342, L-val: 0.9176\n",
            "030: Acc-tr:  93.52, Acc-val:  80.02, L-tr: 0.1849, L-val: 0.8323\n",
            "031: Acc-tr:  89.95, Acc-val:  77.43, L-tr: 0.3106, L-val: 1.0057\n",
            "032: Acc-tr:  93.12, Acc-val:  80.04, L-tr: 0.1967, L-val: 0.7901\n",
            "033: Acc-tr:  95.24, Acc-val:  81.62, L-tr: 0.1358, L-val: 0.7415\n",
            "034: Acc-tr:  93.50, Acc-val:  79.46, L-tr: 0.1886, L-val: 0.8582\n",
            "035: Acc-tr:  94.29, Acc-val:  80.25, L-tr: 0.1622, L-val: 0.7718\n",
            "036: Acc-tr:  92.06, Acc-val:  77.89, L-tr: 0.2180, L-val: 0.8784\n",
            "037: Acc-tr:  95.71, Acc-val:  81.69, L-tr: 0.1267, L-val: 0.6881\n",
            "038: Acc-tr:  96.55, Acc-val:  81.97, L-tr: 0.0980, L-val: 0.7254\n",
            "039: Acc-tr:  94.29, Acc-val:  79.21, L-tr: 0.1594, L-val: 0.8497\n",
            "040: Acc-tr:  93.76, Acc-val:  79.70, L-tr: 0.1742, L-val: 0.8325\n",
            "041: Acc-tr:  92.26, Acc-val:  77.81, L-tr: 0.2278, L-val: 0.9232\n",
            "042: Acc-tr:  94.70, Acc-val:  80.78, L-tr: 0.1498, L-val: 0.8141\n",
            "043: Acc-tr:  96.82, Acc-val:  82.07, L-tr: 0.0916, L-val: 0.7295\n",
            "044: Acc-tr:  92.00, Acc-val:  77.77, L-tr: 0.2391, L-val: 0.9889\n",
            "045: Acc-tr:  94.67, Acc-val:  79.92, L-tr: 0.1500, L-val: 0.8038\n",
            "046: Acc-tr:  95.76, Acc-val:  81.15, L-tr: 0.1217, L-val: 0.7554\n",
            "047: Acc-tr:  94.69, Acc-val:  80.47, L-tr: 0.1542, L-val: 0.7984\n",
            "048: Acc-tr:  95.91, Acc-val:  80.84, L-tr: 0.1135, L-val: 0.7647\n",
            "049: Acc-tr:  95.46, Acc-val:  80.82, L-tr: 0.1294, L-val: 0.8471\n",
            "050: Acc-tr:  95.20, Acc-val:  80.64, L-tr: 0.1402, L-val: 0.8252\n",
            "051: Acc-tr:  92.96, Acc-val:  79.21, L-tr: 0.2118, L-val: 0.9569\n",
            "052: Acc-tr:  90.72, Acc-val:  76.76, L-tr: 0.2890, L-val: 1.0256\n",
            "053: Acc-tr:  96.57, Acc-val:  81.57, L-tr: 0.0978, L-val: 0.7368\n",
            "054: Acc-tr:  95.73, Acc-val:  81.29, L-tr: 0.1187, L-val: 0.7769\n",
            "055: Acc-tr:  95.09, Acc-val:  80.54, L-tr: 0.1370, L-val: 0.7890\n",
            "056: Acc-tr:  94.05, Acc-val:  80.23, L-tr: 0.1703, L-val: 0.8112\n",
            "057: Acc-tr:  95.90, Acc-val:  80.59, L-tr: 0.1174, L-val: 0.8055\n",
            "058: Acc-tr:  96.03, Acc-val:  81.10, L-tr: 0.1146, L-val: 0.7566\n",
            "059: Acc-tr:  96.13, Acc-val:  81.68, L-tr: 0.1079, L-val: 0.7992\n",
            "060: Acc-tr:  96.10, Acc-val:  81.47, L-tr: 0.1085, L-val: 0.8203\n",
            "061: Acc-tr:  95.63, Acc-val:  81.02, L-tr: 0.1236, L-val: 0.7757\n",
            "062: Acc-tr:  92.45, Acc-val:  79.59, L-tr: 0.2187, L-val: 0.8964\n",
            "063: Acc-tr:  95.11, Acc-val:  80.04, L-tr: 0.1395, L-val: 0.7807\n",
            "064: Acc-tr:  91.17, Acc-val:  78.56, L-tr: 0.3033, L-val: 1.0922\n",
            "065: Acc-tr:  93.48, Acc-val:  79.83, L-tr: 0.1951, L-val: 0.9342\n",
            "066: Acc-tr:  95.89, Acc-val:  81.26, L-tr: 0.1150, L-val: 0.7701\n",
            "067: Acc-tr:  95.19, Acc-val:  81.01, L-tr: 0.1376, L-val: 0.8071\n",
            "068: Acc-tr:  91.28, Acc-val:  77.94, L-tr: 0.2626, L-val: 0.9787\n",
            "069: Acc-tr:  96.58, Acc-val:  81.99, L-tr: 0.0973, L-val: 0.7249\n",
            "070: Acc-tr:  94.60, Acc-val:  80.13, L-tr: 0.1535, L-val: 0.8374\n",
            "071: Acc-tr:  96.88, Acc-val:  82.64, L-tr: 0.0884, L-val: 0.7072\n",
            "072: Acc-tr:  94.56, Acc-val:  80.31, L-tr: 0.1630, L-val: 0.8861\n",
            "073: Acc-tr:  97.13, Acc-val:  81.79, L-tr: 0.0865, L-val: 0.7192\n",
            "074: Acc-tr:  95.64, Acc-val:  80.56, L-tr: 0.1245, L-val: 0.7966\n",
            "075: Acc-tr:  94.81, Acc-val:  80.83, L-tr: 0.1473, L-val: 0.8157\n",
            "076: Acc-tr:  92.44, Acc-val:  77.51, L-tr: 0.2323, L-val: 1.0066\n",
            "077: Acc-tr:  92.16, Acc-val:  77.36, L-tr: 0.2469, L-val: 1.0178\n",
            "078: Acc-tr:  95.61, Acc-val:  80.59, L-tr: 0.1266, L-val: 0.8107\n",
            "079: Acc-tr:  94.72, Acc-val:  80.00, L-tr: 0.1526, L-val: 0.8410\n",
            "080: Acc-tr:  96.40, Acc-val:  81.39, L-tr: 0.1006, L-val: 0.7940\n",
            "081: Acc-tr:  96.77, Acc-val:  82.34, L-tr: 0.0891, L-val: 0.7775\n",
            "082: Acc-tr:  94.50, Acc-val:  79.84, L-tr: 0.1538, L-val: 0.8207\n",
            "083: Acc-tr:  91.97, Acc-val:  78.59, L-tr: 0.2382, L-val: 0.9942\n",
            "084: Acc-tr:  96.45, Acc-val:  82.02, L-tr: 0.1024, L-val: 0.7460\n",
            "085: Acc-tr:  96.42, Acc-val:  81.22, L-tr: 0.1044, L-val: 0.8092\n",
            "086: Acc-tr:  96.18, Acc-val:  81.35, L-tr: 0.1055, L-val: 0.7434\n",
            "087: Acc-tr:  95.19, Acc-val:  80.98, L-tr: 0.1371, L-val: 0.8057\n",
            "088: Acc-tr:  94.94, Acc-val:  80.51, L-tr: 0.1506, L-val: 0.8753\n",
            "089: Acc-tr:  96.26, Acc-val:  81.70, L-tr: 0.1075, L-val: 0.8063\n",
            "090: Acc-tr:  93.59, Acc-val:  79.93, L-tr: 0.1836, L-val: 0.8858\n",
            "091: Acc-tr:  95.78, Acc-val:  80.98, L-tr: 0.1258, L-val: 0.8233\n",
            "092: Acc-tr:  96.00, Acc-val:  81.19, L-tr: 0.1154, L-val: 0.8198\n",
            "093: Acc-tr:  96.44, Acc-val:  81.52, L-tr: 0.1035, L-val: 0.7784\n",
            "094: Acc-tr:  96.58, Acc-val:  82.09, L-tr: 0.0977, L-val: 0.7128\n",
            "095: Acc-tr:  96.37, Acc-val:  81.27, L-tr: 0.1008, L-val: 0.7877\n",
            "096: Acc-tr:  93.22, Acc-val:  79.11, L-tr: 0.1993, L-val: 0.9381\n",
            "097: Acc-tr:  96.76, Acc-val:  82.02, L-tr: 0.0927, L-val: 0.7446\n",
            "098: Acc-tr:  94.06, Acc-val:  79.11, L-tr: 0.1717, L-val: 0.9301\n",
            "099: Acc-tr:  94.56, Acc-val:  80.84, L-tr: 0.1643, L-val: 0.8843\n",
            "100: Acc-tr:  95.49, Acc-val:  80.53, L-tr: 0.1295, L-val: 0.8381\n",
            "101: Acc-tr:  94.96, Acc-val:  80.57, L-tr: 0.1465, L-val: 0.8361\n",
            "102: Acc-tr:  92.28, Acc-val:  78.70, L-tr: 0.2302, L-val: 0.9650\n",
            "103: Acc-tr:  93.23, Acc-val:  79.20, L-tr: 0.1979, L-val: 0.9144\n",
            "104: Acc-tr:  93.49, Acc-val:  78.95, L-tr: 0.1834, L-val: 0.8799\n",
            "105: Acc-tr:  96.47, Acc-val:  81.53, L-tr: 0.1033, L-val: 0.7638\n",
            "106: Acc-tr:  95.67, Acc-val:  80.95, L-tr: 0.1238, L-val: 0.7619\n",
            "107: Acc-tr:  96.23, Acc-val:  81.34, L-tr: 0.1066, L-val: 0.7552\n",
            "108: Acc-tr:  94.10, Acc-val:  79.06, L-tr: 0.1756, L-val: 0.9828\n",
            "109: Acc-tr:  96.34, Acc-val:  81.61, L-tr: 0.1035, L-val: 0.7662\n",
            "110: Acc-tr:  93.57, Acc-val:  79.66, L-tr: 0.1889, L-val: 0.9037\n",
            "111: Acc-tr:  95.20, Acc-val:  80.08, L-tr: 0.1350, L-val: 0.9129\n",
            "112: Acc-tr:  94.11, Acc-val:  79.28, L-tr: 0.1773, L-val: 0.9196\n",
            "113: Acc-tr:  97.87, Acc-val:  83.15, L-tr: 0.0637, L-val: 0.6805\n",
            "114: Acc-tr:  97.49, Acc-val:  82.69, L-tr: 0.0730, L-val: 0.6931\n",
            "115: Acc-tr:  96.43, Acc-val:  80.99, L-tr: 0.1014, L-val: 0.7856\n",
            "116: Acc-tr:  88.94, Acc-val:  75.30, L-tr: 0.3452, L-val: 1.1171\n",
            "117: Acc-tr:  96.24, Acc-val:  81.79, L-tr: 0.1068, L-val: 0.7968\n",
            "118: Acc-tr:  92.88, Acc-val:  78.68, L-tr: 0.2052, L-val: 0.9334\n",
            "119: Acc-tr:  95.07, Acc-val:  80.06, L-tr: 0.1436, L-val: 0.8263\n",
            "120: Acc-tr:  95.98, Acc-val:  81.66, L-tr: 0.1177, L-val: 0.8009\n",
            "121: Acc-tr:  96.07, Acc-val:  81.65, L-tr: 0.1127, L-val: 0.7554\n",
            "122: Acc-tr:  94.80, Acc-val:  79.25, L-tr: 0.1519, L-val: 0.9007\n",
            "123: Acc-tr:  96.32, Acc-val:  81.47, L-tr: 0.1021, L-val: 0.7849\n",
            "124: Acc-tr:  97.01, Acc-val:  82.10, L-tr: 0.0840, L-val: 0.7760\n",
            "125: Acc-tr:  94.50, Acc-val:  80.86, L-tr: 0.1607, L-val: 0.7864\n",
            "126: Acc-tr:  94.15, Acc-val:  79.03, L-tr: 0.1742, L-val: 0.9684\n",
            "127: Acc-tr:  95.13, Acc-val:  79.75, L-tr: 0.1447, L-val: 0.9483\n",
            "128: Acc-tr:  93.41, Acc-val:  78.28, L-tr: 0.1897, L-val: 0.8924\n",
            "129: Acc-tr:  96.68, Acc-val:  81.67, L-tr: 0.0956, L-val: 0.7381\n",
            "130: Acc-tr:  96.20, Acc-val:  81.77, L-tr: 0.1089, L-val: 0.7750\n",
            "131: Acc-tr:  96.16, Acc-val:  82.08, L-tr: 0.1155, L-val: 0.7787\n",
            "132: Acc-tr:  95.93, Acc-val:  81.29, L-tr: 0.1148, L-val: 0.7819\n",
            "133: Acc-tr:  93.16, Acc-val:  79.05, L-tr: 0.2117, L-val: 1.0187\n",
            "134: Acc-tr:  95.03, Acc-val:  81.07, L-tr: 0.1457, L-val: 0.8558\n",
            "135: Acc-tr:  95.91, Acc-val:  81.49, L-tr: 0.1175, L-val: 0.7809\n",
            "136: Acc-tr:  96.38, Acc-val:  82.00, L-tr: 0.1060, L-val: 0.7541\n",
            "137: Acc-tr:  96.25, Acc-val:  81.26, L-tr: 0.1033, L-val: 0.7788\n",
            "138: Acc-tr:  96.50, Acc-val:  81.55, L-tr: 0.1021, L-val: 0.8006\n",
            "139: Acc-tr:  96.50, Acc-val:  82.08, L-tr: 0.1001, L-val: 0.7291\n",
            "140: Acc-tr:  96.27, Acc-val:  81.34, L-tr: 0.1089, L-val: 0.7835\n",
            "141: Acc-tr:  88.31, Acc-val:  75.08, L-tr: 0.4031, L-val: 1.1788\n",
            "142: Acc-tr:  96.42, Acc-val:  81.72, L-tr: 0.1020, L-val: 0.7545\n",
            "143: Acc-tr:  94.48, Acc-val:  80.60, L-tr: 0.1572, L-val: 0.8185\n",
            "144: Acc-tr:  94.97, Acc-val:  80.30, L-tr: 0.1440, L-val: 0.8852\n",
            "145: Acc-tr:  95.96, Acc-val:  80.80, L-tr: 0.1164, L-val: 0.8132\n",
            "146: Acc-tr:  97.01, Acc-val:  81.85, L-tr: 0.0854, L-val: 0.7278\n",
            "147: Acc-tr:  95.85, Acc-val:  81.02, L-tr: 0.1245, L-val: 0.7798\n",
            "148: Acc-tr:  91.46, Acc-val:  77.65, L-tr: 0.2425, L-val: 1.0155\n",
            "149: Acc-tr:  96.64, Acc-val:  81.99, L-tr: 0.0978, L-val: 0.7434\n",
            "150: Acc-tr:  99.99, Acc-val:  85.66, L-tr: 0.0050, L-val: 0.5768\n",
            "151: Acc-tr: 100.00, Acc-val:  85.70, L-tr: 0.0029, L-val: 0.5794\n",
            "152: Acc-tr: 100.00, Acc-val:  85.92, L-tr: 0.0022, L-val: 0.5815\n",
            "153: Acc-tr: 100.00, Acc-val:  85.93, L-tr: 0.0018, L-val: 0.5870\n",
            "154: Acc-tr: 100.00, Acc-val:  85.95, L-tr: 0.0016, L-val: 0.5834\n",
            "155: Acc-tr: 100.00, Acc-val:  85.87, L-tr: 0.0013, L-val: 0.5938\n",
            "156: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0012, L-val: 0.5914\n",
            "157: Acc-tr: 100.00, Acc-val:  86.00, L-tr: 0.0011, L-val: 0.5947\n",
            "158: Acc-tr: 100.00, Acc-val:  86.02, L-tr: 0.0010, L-val: 0.5967\n",
            "159: Acc-tr: 100.00, Acc-val:  86.20, L-tr: 0.0009, L-val: 0.5936\n",
            "160: Acc-tr: 100.00, Acc-val:  86.13, L-tr: 0.0009, L-val: 0.5950\n",
            "161: Acc-tr: 100.00, Acc-val:  86.02, L-tr: 0.0008, L-val: 0.5945\n",
            "162: Acc-tr: 100.00, Acc-val:  86.19, L-tr: 0.0008, L-val: 0.5953\n",
            "163: Acc-tr: 100.00, Acc-val:  86.24, L-tr: 0.0008, L-val: 0.5939\n",
            "164: Acc-tr: 100.00, Acc-val:  86.21, L-tr: 0.0007, L-val: 0.5937\n",
            "165: Acc-tr: 100.00, Acc-val:  86.28, L-tr: 0.0007, L-val: 0.5945\n",
            "166: Acc-tr: 100.00, Acc-val:  86.26, L-tr: 0.0007, L-val: 0.5960\n",
            "167: Acc-tr: 100.00, Acc-val:  86.15, L-tr: 0.0006, L-val: 0.5972\n",
            "168: Acc-tr: 100.00, Acc-val:  86.15, L-tr: 0.0006, L-val: 0.5993\n",
            "169: Acc-tr: 100.00, Acc-val:  86.38, L-tr: 0.0006, L-val: 0.5952\n",
            "170: Acc-tr: 100.00, Acc-val:  86.19, L-tr: 0.0006, L-val: 0.5950\n",
            "171: Acc-tr: 100.00, Acc-val:  86.26, L-tr: 0.0005, L-val: 0.5959\n",
            "172: Acc-tr: 100.00, Acc-val:  86.27, L-tr: 0.0006, L-val: 0.5943\n",
            "173: Acc-tr: 100.00, Acc-val:  86.23, L-tr: 0.0006, L-val: 0.5922\n",
            "174: Acc-tr: 100.00, Acc-val:  86.34, L-tr: 0.0005, L-val: 0.5938\n",
            "175: Acc-tr: 100.00, Acc-val:  86.17, L-tr: 0.0005, L-val: 0.5958\n",
            "176: Acc-tr: 100.00, Acc-val:  86.26, L-tr: 0.0005, L-val: 0.5887\n",
            "177: Acc-tr: 100.00, Acc-val:  86.31, L-tr: 0.0005, L-val: 0.5914\n",
            "178: Acc-tr: 100.00, Acc-val:  86.23, L-tr: 0.0005, L-val: 0.5921\n",
            "179: Acc-tr: 100.00, Acc-val:  86.35, L-tr: 0.0005, L-val: 0.5915\n",
            "180: Acc-tr: 100.00, Acc-val:  86.15, L-tr: 0.0005, L-val: 0.5911\n",
            "181: Acc-tr: 100.00, Acc-val:  86.38, L-tr: 0.0005, L-val: 0.5899\n",
            "182: Acc-tr: 100.00, Acc-val:  86.28, L-tr: 0.0005, L-val: 0.5847\n",
            "183: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0005, L-val: 0.5854\n",
            "184: Acc-tr: 100.00, Acc-val:  86.13, L-tr: 0.0005, L-val: 0.5831\n",
            "185: Acc-tr: 100.00, Acc-val:  86.36, L-tr: 0.0005, L-val: 0.5852\n",
            "186: Acc-tr: 100.00, Acc-val:  86.25, L-tr: 0.0005, L-val: 0.5812\n",
            "187: Acc-tr: 100.00, Acc-val:  86.38, L-tr: 0.0005, L-val: 0.5852\n",
            "188: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0005, L-val: 0.5827\n",
            "189: Acc-tr: 100.00, Acc-val:  86.23, L-tr: 0.0005, L-val: 0.5829\n",
            "190: Acc-tr: 100.00, Acc-val:  86.31, L-tr: 0.0005, L-val: 0.5803\n",
            "191: Acc-tr: 100.00, Acc-val:  86.18, L-tr: 0.0005, L-val: 0.5823\n",
            "192: Acc-tr: 100.00, Acc-val:  86.24, L-tr: 0.0005, L-val: 0.5814\n",
            "193: Acc-tr: 100.00, Acc-val:  86.29, L-tr: 0.0004, L-val: 0.5827\n",
            "194: Acc-tr: 100.00, Acc-val:  86.23, L-tr: 0.0005, L-val: 0.5782\n",
            "195: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0005, L-val: 0.5767\n",
            "196: Acc-tr: 100.00, Acc-val:  86.20, L-tr: 0.0004, L-val: 0.5816\n",
            "197: Acc-tr: 100.00, Acc-val:  86.30, L-tr: 0.0004, L-val: 0.5800\n",
            "198: Acc-tr: 100.00, Acc-val:  86.19, L-tr: 0.0005, L-val: 0.5750\n",
            "199: Acc-tr: 100.00, Acc-val:  86.37, L-tr: 0.0005, L-val: 0.5737\n",
            "200: Acc-tr: 100.00, Acc-val:  86.37, L-tr: 0.0005, L-val: 0.5744\n",
            "201: Acc-tr: 100.00, Acc-val:  86.31, L-tr: 0.0004, L-val: 0.5744\n",
            "202: Acc-tr: 100.00, Acc-val:  86.26, L-tr: 0.0005, L-val: 0.5739\n",
            "203: Acc-tr: 100.00, Acc-val:  86.36, L-tr: 0.0005, L-val: 0.5715\n",
            "204: Acc-tr: 100.00, Acc-val:  86.27, L-tr: 0.0005, L-val: 0.5713\n",
            "205: Acc-tr: 100.00, Acc-val:  86.24, L-tr: 0.0005, L-val: 0.5705\n",
            "206: Acc-tr: 100.00, Acc-val:  86.19, L-tr: 0.0004, L-val: 0.5762\n",
            "207: Acc-tr: 100.00, Acc-val:  86.31, L-tr: 0.0004, L-val: 0.5724\n",
            "208: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0005, L-val: 0.5707\n",
            "209: Acc-tr: 100.00, Acc-val:  86.29, L-tr: 0.0004, L-val: 0.5686\n",
            "210: Acc-tr: 100.00, Acc-val:  86.23, L-tr: 0.0004, L-val: 0.5721\n",
            "211: Acc-tr: 100.00, Acc-val:  86.22, L-tr: 0.0005, L-val: 0.5713\n",
            "212: Acc-tr: 100.00, Acc-val:  86.26, L-tr: 0.0005, L-val: 0.5679\n",
            "213: Acc-tr: 100.00, Acc-val:  86.12, L-tr: 0.0004, L-val: 0.5734\n",
            "214: Acc-tr: 100.00, Acc-val:  86.28, L-tr: 0.0004, L-val: 0.5658\n",
            "215: Acc-tr: 100.00, Acc-val:  86.34, L-tr: 0.0005, L-val: 0.5636\n",
            "216: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0004, L-val: 0.5649\n",
            "217: Acc-tr: 100.00, Acc-val:  86.29, L-tr: 0.0004, L-val: 0.5643\n",
            "218: Acc-tr: 100.00, Acc-val:  86.24, L-tr: 0.0004, L-val: 0.5659\n",
            "219: Acc-tr: 100.00, Acc-val:  86.26, L-tr: 0.0005, L-val: 0.5622\n",
            "220: Acc-tr: 100.00, Acc-val:  86.45, L-tr: 0.0005, L-val: 0.5623\n",
            "221: Acc-tr: 100.00, Acc-val:  86.23, L-tr: 0.0005, L-val: 0.5653\n",
            "222: Acc-tr: 100.00, Acc-val:  86.37, L-tr: 0.0005, L-val: 0.5583\n",
            "223: Acc-tr: 100.00, Acc-val:  86.24, L-tr: 0.0004, L-val: 0.5603\n",
            "224: Acc-tr: 100.00, Acc-val:  86.28, L-tr: 0.0004, L-val: 0.5579\n",
            "225: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0004, L-val: 0.5597\n",
            "226: Acc-tr: 100.00, Acc-val:  86.32, L-tr: 0.0004, L-val: 0.5620\n",
            "227: Acc-tr: 100.00, Acc-val:  86.31, L-tr: 0.0004, L-val: 0.5609\n",
            "228: Acc-tr: 100.00, Acc-val:  86.32, L-tr: 0.0005, L-val: 0.5610\n",
            "229: Acc-tr: 100.00, Acc-val:  86.24, L-tr: 0.0005, L-val: 0.5607\n",
            "230: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0004, L-val: 0.5569\n",
            "231: Acc-tr: 100.00, Acc-val:  86.32, L-tr: 0.0004, L-val: 0.5617\n",
            "232: Acc-tr: 100.00, Acc-val:  86.29, L-tr: 0.0005, L-val: 0.5609\n",
            "233: Acc-tr: 100.00, Acc-val:  86.28, L-tr: 0.0004, L-val: 0.5613\n",
            "234: Acc-tr: 100.00, Acc-val:  86.38, L-tr: 0.0004, L-val: 0.5589\n",
            "235: Acc-tr: 100.00, Acc-val:  86.28, L-tr: 0.0004, L-val: 0.5594\n",
            "236: Acc-tr: 100.00, Acc-val:  86.29, L-tr: 0.0004, L-val: 0.5627\n",
            "237: Acc-tr: 100.00, Acc-val:  86.39, L-tr: 0.0004, L-val: 0.5596\n",
            "238: Acc-tr: 100.00, Acc-val:  86.32, L-tr: 0.0004, L-val: 0.5591\n",
            "239: Acc-tr: 100.00, Acc-val:  86.34, L-tr: 0.0004, L-val: 0.5589\n",
            "240: Acc-tr: 100.00, Acc-val:  86.32, L-tr: 0.0004, L-val: 0.5592\n",
            "241: Acc-tr: 100.00, Acc-val:  86.40, L-tr: 0.0004, L-val: 0.5580\n",
            "242: Acc-tr: 100.00, Acc-val:  86.31, L-tr: 0.0004, L-val: 0.5594\n",
            "243: Acc-tr: 100.00, Acc-val:  86.35, L-tr: 0.0005, L-val: 0.5583\n",
            "244: Acc-tr: 100.00, Acc-val:  86.40, L-tr: 0.0004, L-val: 0.5590\n",
            "245: Acc-tr: 100.00, Acc-val:  86.21, L-tr: 0.0004, L-val: 0.5620\n",
            "246: Acc-tr: 100.00, Acc-val:  86.41, L-tr: 0.0004, L-val: 0.5607\n",
            "247: Acc-tr: 100.00, Acc-val:  86.32, L-tr: 0.0004, L-val: 0.5610\n",
            "248: Acc-tr: 100.00, Acc-val:  86.34, L-tr: 0.0004, L-val: 0.5571\n",
            "249: Acc-tr: 100.00, Acc-val:  86.18, L-tr: 0.0004, L-val: 0.5635\n",
            "250: Acc-tr: 100.00, Acc-val:  86.39, L-tr: 0.0005, L-val: 0.5577\n",
            "251: Acc-tr: 100.00, Acc-val:  86.39, L-tr: 0.0005, L-val: 0.5589\n",
            "252: Acc-tr: 100.00, Acc-val:  86.29, L-tr: 0.0004, L-val: 0.5590\n",
            "253: Acc-tr: 100.00, Acc-val:  86.48, L-tr: 0.0004, L-val: 0.5584\n",
            "254: Acc-tr: 100.00, Acc-val:  86.35, L-tr: 0.0004, L-val: 0.5592\n",
            "255: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0004, L-val: 0.5595\n",
            "256: Acc-tr: 100.00, Acc-val:  86.44, L-tr: 0.0004, L-val: 0.5582\n",
            "257: Acc-tr: 100.00, Acc-val:  86.29, L-tr: 0.0004, L-val: 0.5624\n",
            "258: Acc-tr: 100.00, Acc-val:  86.30, L-tr: 0.0004, L-val: 0.5570\n",
            "259: Acc-tr: 100.00, Acc-val:  86.35, L-tr: 0.0004, L-val: 0.5603\n",
            "260: Acc-tr: 100.00, Acc-val:  86.40, L-tr: 0.0004, L-val: 0.5611\n",
            "261: Acc-tr: 100.00, Acc-val:  86.26, L-tr: 0.0005, L-val: 0.5589\n",
            "262: Acc-tr: 100.00, Acc-val:  86.36, L-tr: 0.0004, L-val: 0.5564\n",
            "263: Acc-tr: 100.00, Acc-val:  86.39, L-tr: 0.0004, L-val: 0.5587\n",
            "264: Acc-tr: 100.00, Acc-val:  86.34, L-tr: 0.0005, L-val: 0.5573\n",
            "265: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0004, L-val: 0.5586\n",
            "266: Acc-tr: 100.00, Acc-val:  86.43, L-tr: 0.0004, L-val: 0.5580\n",
            "267: Acc-tr: 100.00, Acc-val:  86.38, L-tr: 0.0005, L-val: 0.5548\n",
            "268: Acc-tr: 100.00, Acc-val:  86.34, L-tr: 0.0004, L-val: 0.5578\n",
            "269: Acc-tr: 100.00, Acc-val:  86.44, L-tr: 0.0004, L-val: 0.5580\n",
            "270: Acc-tr: 100.00, Acc-val:  86.35, L-tr: 0.0005, L-val: 0.5575\n",
            "271: Acc-tr: 100.00, Acc-val:  86.36, L-tr: 0.0004, L-val: 0.5581\n",
            "272: Acc-tr: 100.00, Acc-val:  86.28, L-tr: 0.0005, L-val: 0.5551\n",
            "273: Acc-tr: 100.00, Acc-val:  86.36, L-tr: 0.0004, L-val: 0.5567\n",
            "274: Acc-tr: 100.00, Acc-val:  86.32, L-tr: 0.0005, L-val: 0.5557\n",
            "275: Acc-tr: 100.00, Acc-val:  86.32, L-tr: 0.0004, L-val: 0.5574\n",
            "276: Acc-tr: 100.00, Acc-val:  86.36, L-tr: 0.0004, L-val: 0.5568\n",
            "277: Acc-tr: 100.00, Acc-val:  86.38, L-tr: 0.0005, L-val: 0.5589\n",
            "278: Acc-tr: 100.00, Acc-val:  86.36, L-tr: 0.0004, L-val: 0.5549\n",
            "279: Acc-tr: 100.00, Acc-val:  86.32, L-tr: 0.0004, L-val: 0.5586\n",
            "280: Acc-tr: 100.00, Acc-val:  86.40, L-tr: 0.0004, L-val: 0.5559\n",
            "281: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0004, L-val: 0.5601\n",
            "282: Acc-tr: 100.00, Acc-val:  86.40, L-tr: 0.0004, L-val: 0.5594\n",
            "283: Acc-tr: 100.00, Acc-val:  86.36, L-tr: 0.0004, L-val: 0.5627\n",
            "284: Acc-tr: 100.00, Acc-val:  86.35, L-tr: 0.0004, L-val: 0.5587\n",
            "285: Acc-tr: 100.00, Acc-val:  86.45, L-tr: 0.0004, L-val: 0.5564\n",
            "286: Acc-tr: 100.00, Acc-val:  86.35, L-tr: 0.0005, L-val: 0.5571\n",
            "287: Acc-tr: 100.00, Acc-val:  86.38, L-tr: 0.0004, L-val: 0.5544\n",
            "288: Acc-tr: 100.00, Acc-val:  86.52, L-tr: 0.0004, L-val: 0.5556\n",
            "289: Acc-tr: 100.00, Acc-val:  86.34, L-tr: 0.0004, L-val: 0.5612\n",
            "290: Acc-tr: 100.00, Acc-val:  86.42, L-tr: 0.0005, L-val: 0.5534\n",
            "291: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0004, L-val: 0.5573\n",
            "292: Acc-tr: 100.00, Acc-val:  86.28, L-tr: 0.0005, L-val: 0.5594\n",
            "293: Acc-tr: 100.00, Acc-val:  86.29, L-tr: 0.0004, L-val: 0.5553\n",
            "294: Acc-tr: 100.00, Acc-val:  86.33, L-tr: 0.0005, L-val: 0.5539\n",
            "295: Acc-tr: 100.00, Acc-val:  86.24, L-tr: 0.0004, L-val: 0.5574\n",
            "296: Acc-tr: 100.00, Acc-val:  86.30, L-tr: 0.0004, L-val: 0.5605\n",
            "297: Acc-tr: 100.00, Acc-val:  86.37, L-tr: 0.0005, L-val: 0.5595\n",
            "298: Acc-tr: 100.00, Acc-val:  86.37, L-tr: 0.0004, L-val: 0.5590\n",
            "299: Acc-tr: 100.00, Acc-val:  86.34, L-tr: 0.0004, L-val: 0.5579\n",
            "<Figure size 640x480 with 2 Axes>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs5uoVo00Qiu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5475
        },
        "outputId": "49844c07-bf51-4922-87fd-4480fa391bcb"
      },
      "source": [
        "!python train.py "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(arch='wide-resnet', batch_size=128, command='train', data='cifar10', data_augmentation=False, epochs=300, eval_full_trainset=True, exp_name='cifar10_wide-resnet28-1_lr0.1_mmt0.9_Wd0.0001_NoAug', label_corrupt_prob=0.0, learning_rate=0.1, mlp_spec='512', momentum=0.9, name='', num_classes=10, weight_decay=0.0001, wrn_depth=28, wrn_droprate=0.0, wrn_widen_factor=1)\n",
            "Logging into runs/cifar10_wide-resnet28-1_lr0.1_mmt0.9_Wd0.0001_NoAug...\n",
            "Files already downloaded and verified\n",
            "Number of parameters: 369498\n",
            "train.py:259: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input_var = torch.autograd.Variable(input, volatile=True)\n",
            "train.py:260: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  target_var = torch.autograd.Variable(target, volatile=True)\n",
            "000: Acc-tr:  61.60, Acc-val:  61.05, L-tr: 1.0634, L-val: 1.0958\n",
            "001: Acc-tr:  69.36, Acc-val:  66.62, L-tr: 0.8749, L-val: 0.9485\n",
            "002: Acc-tr:  71.29, Acc-val:  68.66, L-tr: 0.8570, L-val: 0.9382\n",
            "003: Acc-tr:  76.68, Acc-val:  72.94, L-tr: 0.6839, L-val: 0.8149\n",
            "004: Acc-tr:  82.17, Acc-val:  77.22, L-tr: 0.5168, L-val: 0.6744\n",
            "005: Acc-tr:  83.92, Acc-val:  77.87, L-tr: 0.4650, L-val: 0.6476\n",
            "006: Acc-tr:  83.79, Acc-val:  77.82, L-tr: 0.4829, L-val: 0.7098\n",
            "007: Acc-tr:  87.06, Acc-val:  79.87, L-tr: 0.3746, L-val: 0.6391\n",
            "008: Acc-tr:  86.91, Acc-val:  78.62, L-tr: 0.3833, L-val: 0.6884\n",
            "009: Acc-tr:  84.67, Acc-val:  76.83, L-tr: 0.4563, L-val: 0.8075\n",
            "010: Acc-tr:  83.30, Acc-val:  75.32, L-tr: 0.4975, L-val: 0.8551\n",
            "011: Acc-tr:  87.90, Acc-val:  78.35, L-tr: 0.3368, L-val: 0.7400\n",
            "012: Acc-tr:  87.86, Acc-val:  78.25, L-tr: 0.3413, L-val: 0.7556\n",
            "013: Acc-tr:  93.02, Acc-val:  81.46, L-tr: 0.1979, L-val: 0.6048\n",
            "014: Acc-tr:  93.79, Acc-val:  82.02, L-tr: 0.1759, L-val: 0.6041\n",
            "015: Acc-tr:  91.03, Acc-val:  79.94, L-tr: 0.2439, L-val: 0.6862\n",
            "016: Acc-tr:  89.85, Acc-val:  78.26, L-tr: 0.2911, L-val: 0.7828\n",
            "017: Acc-tr:  92.42, Acc-val:  80.17, L-tr: 0.2176, L-val: 0.7575\n",
            "018: Acc-tr:  91.60, Acc-val:  79.60, L-tr: 0.2411, L-val: 0.7680\n",
            "019: Acc-tr:  91.20, Acc-val:  79.25, L-tr: 0.2437, L-val: 0.8110\n",
            "020: Acc-tr:  94.84, Acc-val:  81.69, L-tr: 0.1448, L-val: 0.6862\n",
            "021: Acc-tr:  90.96, Acc-val:  78.68, L-tr: 0.2501, L-val: 0.8543\n",
            "022: Acc-tr:  95.06, Acc-val:  82.20, L-tr: 0.1410, L-val: 0.6658\n",
            "023: Acc-tr:  93.42, Acc-val:  79.87, L-tr: 0.1795, L-val: 0.7587\n",
            "024: Acc-tr:  92.46, Acc-val:  79.30, L-tr: 0.2138, L-val: 0.8181\n",
            "025: Acc-tr:  93.27, Acc-val:  80.45, L-tr: 0.1893, L-val: 0.8115\n",
            "026: Acc-tr:  93.70, Acc-val:  80.31, L-tr: 0.1769, L-val: 0.7538\n",
            "027: Acc-tr:  92.04, Acc-val:  79.14, L-tr: 0.2331, L-val: 0.8479\n",
            "028: Acc-tr:  93.53, Acc-val:  79.96, L-tr: 0.1887, L-val: 0.8022\n",
            "029: Acc-tr:  95.15, Acc-val:  81.31, L-tr: 0.1370, L-val: 0.7402\n",
            "030: Acc-tr:  95.94, Acc-val:  81.76, L-tr: 0.1156, L-val: 0.7148\n",
            "031: Acc-tr:  94.84, Acc-val:  80.86, L-tr: 0.1526, L-val: 0.7792\n",
            "032: Acc-tr:  93.61, Acc-val:  79.66, L-tr: 0.1818, L-val: 0.8155\n",
            "033: Acc-tr:  93.72, Acc-val:  80.33, L-tr: 0.1753, L-val: 0.8101\n",
            "034: Acc-tr:  92.23, Acc-val:  78.03, L-tr: 0.2248, L-val: 0.9363\n",
            "035: Acc-tr:  94.58, Acc-val:  80.29, L-tr: 0.1488, L-val: 0.8281\n",
            "036: Acc-tr:  93.81, Acc-val:  79.60, L-tr: 0.1739, L-val: 0.8407\n",
            "037: Acc-tr:  95.23, Acc-val:  80.40, L-tr: 0.1371, L-val: 0.8078\n",
            "038: Acc-tr:  96.97, Acc-val:  82.60, L-tr: 0.0891, L-val: 0.6633\n",
            "039: Acc-tr:  94.07, Acc-val:  80.41, L-tr: 0.1708, L-val: 0.8692\n",
            "040: Acc-tr:  96.46, Acc-val:  81.92, L-tr: 0.1036, L-val: 0.7363\n",
            "041: Acc-tr:  93.28, Acc-val:  79.34, L-tr: 0.1938, L-val: 0.9179\n",
            "042: Acc-tr:  96.49, Acc-val:  81.33, L-tr: 0.1016, L-val: 0.7896\n",
            "043: Acc-tr:  91.25, Acc-val:  77.57, L-tr: 0.2734, L-val: 1.0357\n",
            "044: Acc-tr:  94.48, Acc-val:  79.78, L-tr: 0.1546, L-val: 0.8533\n",
            "045: Acc-tr:  94.23, Acc-val:  78.92, L-tr: 0.1690, L-val: 0.9170\n",
            "046: Acc-tr:  93.80, Acc-val:  79.78, L-tr: 0.1750, L-val: 0.8721\n",
            "047: Acc-tr:  97.24, Acc-val:  82.83, L-tr: 0.0797, L-val: 0.7220\n",
            "048: Acc-tr:  95.23, Acc-val:  81.39, L-tr: 0.1360, L-val: 0.7763\n",
            "049: Acc-tr:  91.95, Acc-val:  77.15, L-tr: 0.2340, L-val: 0.9451\n",
            "050: Acc-tr:  94.55, Acc-val:  79.97, L-tr: 0.1551, L-val: 0.8656\n",
            "051: Acc-tr:  94.57, Acc-val:  80.64, L-tr: 0.1543, L-val: 0.8221\n",
            "052: Acc-tr:  95.27, Acc-val:  80.41, L-tr: 0.1341, L-val: 0.8102\n",
            "053: Acc-tr:  93.91, Acc-val:  80.11, L-tr: 0.1718, L-val: 0.8040\n",
            "054: Acc-tr:  95.60, Acc-val:  80.89, L-tr: 0.1252, L-val: 0.8144\n",
            "055: Acc-tr:  95.74, Acc-val:  81.14, L-tr: 0.1230, L-val: 0.7662\n",
            "056: Acc-tr:  95.12, Acc-val:  80.33, L-tr: 0.1438, L-val: 0.8709\n",
            "057: Acc-tr:  96.89, Acc-val:  81.84, L-tr: 0.0896, L-val: 0.7381\n",
            "058: Acc-tr:  94.27, Acc-val:  80.25, L-tr: 0.1630, L-val: 0.7859\n",
            "059: Acc-tr:  94.76, Acc-val:  80.11, L-tr: 0.1410, L-val: 0.8165\n",
            "060: Acc-tr:  87.04, Acc-val:  74.44, L-tr: 0.3940, L-val: 1.1188\n",
            "061: Acc-tr:  96.56, Acc-val:  81.65, L-tr: 0.0988, L-val: 0.7576\n",
            "062: Acc-tr:  95.62, Acc-val:  81.06, L-tr: 0.1246, L-val: 0.8400\n",
            "063: Acc-tr:  96.06, Acc-val:  81.10, L-tr: 0.1107, L-val: 0.7676\n",
            "064: Acc-tr:  92.17, Acc-val:  77.91, L-tr: 0.2216, L-val: 0.9558\n",
            "065: Acc-tr:  97.03, Acc-val:  82.01, L-tr: 0.0874, L-val: 0.7513\n",
            "066: Acc-tr:  95.51, Acc-val:  81.00, L-tr: 0.1251, L-val: 0.8203\n",
            "067: Acc-tr:  98.10, Acc-val:  83.55, L-tr: 0.0551, L-val: 0.6994\n",
            "068: Acc-tr:  97.05, Acc-val:  82.37, L-tr: 0.0894, L-val: 0.7874\n",
            "069: Acc-tr:  95.85, Acc-val:  80.72, L-tr: 0.1203, L-val: 0.8125\n",
            "070: Acc-tr:  95.47, Acc-val:  80.53, L-tr: 0.1296, L-val: 0.8035\n",
            "071: Acc-tr:  96.44, Acc-val:  81.41, L-tr: 0.1008, L-val: 0.7349\n",
            "072: Acc-tr:  95.62, Acc-val:  80.52, L-tr: 0.1237, L-val: 0.8163\n",
            "073: Acc-tr:  95.68, Acc-val:  81.25, L-tr: 0.1256, L-val: 0.8016\n",
            "074: Acc-tr:  97.05, Acc-val:  82.38, L-tr: 0.0853, L-val: 0.7265\n",
            "075: Acc-tr:  92.43, Acc-val:  78.40, L-tr: 0.2165, L-val: 0.9135\n",
            "076: Acc-tr:  95.42, Acc-val:  80.83, L-tr: 0.1289, L-val: 0.8051\n",
            "077: Acc-tr:  93.91, Acc-val:  78.86, L-tr: 0.1878, L-val: 0.9726\n",
            "078: Acc-tr:  93.89, Acc-val:  79.74, L-tr: 0.1763, L-val: 0.8602\n",
            "079: Acc-tr:  94.77, Acc-val:  79.79, L-tr: 0.1507, L-val: 0.9127\n",
            "080: Acc-tr:  91.59, Acc-val:  78.01, L-tr: 0.2711, L-val: 1.0994\n",
            "081: Acc-tr:  94.55, Acc-val:  80.52, L-tr: 0.1544, L-val: 0.8725\n",
            "082: Acc-tr:  97.56, Acc-val:  82.22, L-tr: 0.0720, L-val: 0.7617\n",
            "083: Acc-tr:  93.31, Acc-val:  78.76, L-tr: 0.1977, L-val: 0.9275\n",
            "084: Acc-tr:  96.99, Acc-val:  82.03, L-tr: 0.0861, L-val: 0.7278\n",
            "085: Acc-tr:  95.04, Acc-val:  80.13, L-tr: 0.1470, L-val: 0.8670\n",
            "086: Acc-tr:  95.09, Acc-val:  80.77, L-tr: 0.1396, L-val: 0.8122\n",
            "087: Acc-tr:  96.05, Acc-val:  81.17, L-tr: 0.1134, L-val: 0.7976\n",
            "088: Acc-tr:  97.17, Acc-val:  82.33, L-tr: 0.0822, L-val: 0.7329\n",
            "089: Acc-tr:  89.00, Acc-val:  77.24, L-tr: 0.3871, L-val: 1.1784\n",
            "090: Acc-tr:  97.17, Acc-val:  81.58, L-tr: 0.0836, L-val: 0.7009\n",
            "091: Acc-tr:  96.58, Acc-val:  81.77, L-tr: 0.0987, L-val: 0.7755\n",
            "092: Acc-tr:  96.17, Acc-val:  81.25, L-tr: 0.1086, L-val: 0.7715\n",
            "093: Acc-tr:  96.81, Acc-val:  81.40, L-tr: 0.0928, L-val: 0.7663\n",
            "094: Acc-tr:  94.89, Acc-val:  80.64, L-tr: 0.1507, L-val: 0.8553\n",
            "095: Acc-tr:  95.36, Acc-val:  80.30, L-tr: 0.1306, L-val: 0.8685\n",
            "096: Acc-tr:  86.42, Acc-val:  74.48, L-tr: 0.4422, L-val: 1.2673\n",
            "097: Acc-tr:  94.86, Acc-val:  80.32, L-tr: 0.1471, L-val: 0.8274\n",
            "098: Acc-tr:  95.82, Acc-val:  81.13, L-tr: 0.1205, L-val: 0.8251\n",
            "099: Acc-tr:  94.72, Acc-val:  80.44, L-tr: 0.1533, L-val: 0.8676\n",
            "100: Acc-tr:  96.29, Acc-val:  81.50, L-tr: 0.1080, L-val: 0.7473\n",
            "101: Acc-tr:  96.52, Acc-val:  81.81, L-tr: 0.0986, L-val: 0.7931\n",
            "102: Acc-tr:  96.95, Acc-val:  82.47, L-tr: 0.0872, L-val: 0.7272\n",
            "103: Acc-tr:  95.38, Acc-val:  80.25, L-tr: 0.1328, L-val: 0.8683\n",
            "104: Acc-tr:  95.38, Acc-val:  80.63, L-tr: 0.1319, L-val: 0.8004\n",
            "105: Acc-tr:  95.03, Acc-val:  80.46, L-tr: 0.1436, L-val: 0.8192\n",
            "106: Acc-tr:  95.10, Acc-val:  79.74, L-tr: 0.1426, L-val: 0.9041\n",
            "107: Acc-tr:  93.56, Acc-val:  79.51, L-tr: 0.1888, L-val: 0.8981\n",
            "108: Acc-tr:  95.66, Acc-val:  81.03, L-tr: 0.1225, L-val: 0.7901\n",
            "109: Acc-tr:  95.67, Acc-val:  81.12, L-tr: 0.1250, L-val: 0.8170\n",
            "110: Acc-tr:  97.79, Acc-val:  82.90, L-tr: 0.0639, L-val: 0.7270\n",
            "111: Acc-tr:  97.45, Acc-val:  82.63, L-tr: 0.0743, L-val: 0.6983\n",
            "112: Acc-tr:  95.66, Acc-val:  80.66, L-tr: 0.1236, L-val: 0.8431\n",
            "113: Acc-tr:  96.13, Acc-val:  81.08, L-tr: 0.1116, L-val: 0.8416\n",
            "114: Acc-tr:  97.07, Acc-val:  82.48, L-tr: 0.0836, L-val: 0.7480\n",
            "115: Acc-tr:  95.42, Acc-val:  81.03, L-tr: 0.1301, L-val: 0.8048\n",
            "116: Acc-tr:  97.23, Acc-val:  82.33, L-tr: 0.0810, L-val: 0.7392\n",
            "117: Acc-tr:  94.18, Acc-val:  79.64, L-tr: 0.1693, L-val: 0.8839\n",
            "118: Acc-tr:  95.70, Acc-val:  81.06, L-tr: 0.1194, L-val: 0.8283\n",
            "119: Acc-tr:  96.56, Acc-val:  81.53, L-tr: 0.0979, L-val: 0.7776\n",
            "120: Acc-tr:  95.39, Acc-val:  81.41, L-tr: 0.1343, L-val: 0.8339\n",
            "121: Acc-tr:  95.07, Acc-val:  80.68, L-tr: 0.1462, L-val: 0.9162\n",
            "122: Acc-tr:  95.50, Acc-val:  80.84, L-tr: 0.1239, L-val: 0.7735\n",
            "123: Acc-tr:  96.73, Acc-val:  82.35, L-tr: 0.0930, L-val: 0.7300\n",
            "124: Acc-tr:  93.63, Acc-val:  79.04, L-tr: 0.1959, L-val: 0.9433\n",
            "125: Acc-tr:  90.94, Acc-val:  76.71, L-tr: 0.2708, L-val: 1.0702\n",
            "126: Acc-tr:  94.16, Acc-val:  79.61, L-tr: 0.1683, L-val: 0.8877\n",
            "127: Acc-tr:  96.77, Acc-val:  81.94, L-tr: 0.0932, L-val: 0.7624\n",
            "128: Acc-tr:  94.64, Acc-val:  80.18, L-tr: 0.1650, L-val: 0.9517\n",
            "129: Acc-tr:  92.95, Acc-val:  78.33, L-tr: 0.2155, L-val: 0.9335\n",
            "130: Acc-tr:  94.60, Acc-val:  79.82, L-tr: 0.1562, L-val: 0.8882\n",
            "131: Acc-tr:  96.58, Acc-val:  81.78, L-tr: 0.0987, L-val: 0.7381\n",
            "132: Acc-tr:  96.38, Acc-val:  81.25, L-tr: 0.1049, L-val: 0.8171\n",
            "133: Acc-tr:  94.48, Acc-val:  79.62, L-tr: 0.1632, L-val: 0.9138\n",
            "134: Acc-tr:  97.20, Acc-val:  81.25, L-tr: 0.0777, L-val: 0.8021\n",
            "135: Acc-tr:  92.39, Acc-val:  78.37, L-tr: 0.2623, L-val: 1.0670\n",
            "136: Acc-tr:  95.12, Acc-val:  80.06, L-tr: 0.1461, L-val: 0.8756\n",
            "137: Acc-tr:  92.45, Acc-val:  77.78, L-tr: 0.2352, L-val: 0.9980\n",
            "138: Acc-tr:  94.20, Acc-val:  79.35, L-tr: 0.1632, L-val: 0.9206\n",
            "139: Acc-tr:  96.18, Acc-val:  81.61, L-tr: 0.1098, L-val: 0.8017\n",
            "140: Acc-tr:  96.70, Acc-val:  81.82, L-tr: 0.0958, L-val: 0.7577\n",
            "141: Acc-tr:  96.37, Acc-val:  80.81, L-tr: 0.1075, L-val: 0.8551\n",
            "142: Acc-tr:  97.15, Acc-val:  82.32, L-tr: 0.0833, L-val: 0.7624\n",
            "143: Acc-tr:  93.87, Acc-val:  79.21, L-tr: 0.1766, L-val: 0.9093\n",
            "144: Acc-tr:  96.86, Acc-val:  81.66, L-tr: 0.0874, L-val: 0.7784\n",
            "145: Acc-tr:  95.87, Acc-val:  80.69, L-tr: 0.1193, L-val: 0.7876\n",
            "146: Acc-tr:  96.07, Acc-val:  80.67, L-tr: 0.1145, L-val: 0.8287\n",
            "147: Acc-tr:  94.66, Acc-val:  80.08, L-tr: 0.1553, L-val: 0.9099\n",
            "148: Acc-tr:  96.91, Acc-val:  82.02, L-tr: 0.0921, L-val: 0.6984\n",
            "149: Acc-tr:  96.76, Acc-val:  82.19, L-tr: 0.0945, L-val: 0.7523\n",
            "150: Acc-tr:  99.99, Acc-val:  85.65, L-tr: 0.0051, L-val: 0.5839\n",
            "151: Acc-tr: 100.00, Acc-val:  85.74, L-tr: 0.0031, L-val: 0.5936\n",
            "152: Acc-tr: 100.00, Acc-val:  85.83, L-tr: 0.0023, L-val: 0.5948\n",
            "153: Acc-tr: 100.00, Acc-val:  85.78, L-tr: 0.0018, L-val: 0.6017\n",
            "154: Acc-tr: 100.00, Acc-val:  85.76, L-tr: 0.0016, L-val: 0.6048\n",
            "155: Acc-tr: 100.00, Acc-val:  85.77, L-tr: 0.0014, L-val: 0.6136\n",
            "156: Acc-tr: 100.00, Acc-val:  85.85, L-tr: 0.0012, L-val: 0.6146\n",
            "157: Acc-tr: 100.00, Acc-val:  85.70, L-tr: 0.0011, L-val: 0.6097\n",
            "158: Acc-tr: 100.00, Acc-val:  85.68, L-tr: 0.0010, L-val: 0.6122\n",
            "159: Acc-tr: 100.00, Acc-val:  85.85, L-tr: 0.0009, L-val: 0.6131\n",
            "160: Acc-tr: 100.00, Acc-val:  85.82, L-tr: 0.0010, L-val: 0.6165\n",
            "161: Acc-tr: 100.00, Acc-val:  85.99, L-tr: 0.0008, L-val: 0.6168\n",
            "162: Acc-tr: 100.00, Acc-val:  85.92, L-tr: 0.0008, L-val: 0.6169\n",
            "163: Acc-tr: 100.00, Acc-val:  85.94, L-tr: 0.0007, L-val: 0.6162\n",
            "164: Acc-tr: 100.00, Acc-val:  85.85, L-tr: 0.0007, L-val: 0.6187\n",
            "165: Acc-tr: 100.00, Acc-val:  85.91, L-tr: 0.0007, L-val: 0.6190\n",
            "166: Acc-tr: 100.00, Acc-val:  85.85, L-tr: 0.0007, L-val: 0.6162\n",
            "167: Acc-tr: 100.00, Acc-val:  85.80, L-tr: 0.0006, L-val: 0.6191\n",
            "168: Acc-tr: 100.00, Acc-val:  85.92, L-tr: 0.0006, L-val: 0.6222\n",
            "169: Acc-tr: 100.00, Acc-val:  85.82, L-tr: 0.0006, L-val: 0.6154\n",
            "170: Acc-tr: 100.00, Acc-val:  85.83, L-tr: 0.0006, L-val: 0.6150\n",
            "171: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0006, L-val: 0.6226\n",
            "172: Acc-tr: 100.00, Acc-val:  85.90, L-tr: 0.0006, L-val: 0.6164\n",
            "173: Acc-tr: 100.00, Acc-val:  86.04, L-tr: 0.0006, L-val: 0.6140\n",
            "174: Acc-tr: 100.00, Acc-val:  85.84, L-tr: 0.0005, L-val: 0.6180\n",
            "175: Acc-tr: 100.00, Acc-val:  85.94, L-tr: 0.0005, L-val: 0.6152\n",
            "176: Acc-tr: 100.00, Acc-val:  85.94, L-tr: 0.0005, L-val: 0.6143\n",
            "177: Acc-tr: 100.00, Acc-val:  85.97, L-tr: 0.0005, L-val: 0.6091\n",
            "178: Acc-tr: 100.00, Acc-val:  86.04, L-tr: 0.0005, L-val: 0.6095\n",
            "179: Acc-tr: 100.00, Acc-val:  86.07, L-tr: 0.0005, L-val: 0.6083\n",
            "180: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0005, L-val: 0.6129\n",
            "181: Acc-tr: 100.00, Acc-val:  85.96, L-tr: 0.0005, L-val: 0.6106\n",
            "182: Acc-tr: 100.00, Acc-val:  85.94, L-tr: 0.0005, L-val: 0.6117\n",
            "183: Acc-tr: 100.00, Acc-val:  85.86, L-tr: 0.0005, L-val: 0.6114\n",
            "184: Acc-tr: 100.00, Acc-val:  85.84, L-tr: 0.0005, L-val: 0.6072\n",
            "185: Acc-tr: 100.00, Acc-val:  85.97, L-tr: 0.0005, L-val: 0.6082\n",
            "186: Acc-tr: 100.00, Acc-val:  85.96, L-tr: 0.0005, L-val: 0.6077\n",
            "187: Acc-tr: 100.00, Acc-val:  86.08, L-tr: 0.0005, L-val: 0.6038\n",
            "188: Acc-tr: 100.00, Acc-val:  86.13, L-tr: 0.0005, L-val: 0.6075\n",
            "189: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0005, L-val: 0.6050\n",
            "190: Acc-tr: 100.00, Acc-val:  85.88, L-tr: 0.0005, L-val: 0.6058\n",
            "191: Acc-tr: 100.00, Acc-val:  86.06, L-tr: 0.0005, L-val: 0.6009\n",
            "192: Acc-tr: 100.00, Acc-val:  85.92, L-tr: 0.0005, L-val: 0.5994\n",
            "193: Acc-tr: 100.00, Acc-val:  85.96, L-tr: 0.0005, L-val: 0.6026\n",
            "194: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0004, L-val: 0.6030\n",
            "195: Acc-tr: 100.00, Acc-val:  86.08, L-tr: 0.0005, L-val: 0.5994\n",
            "196: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0005, L-val: 0.6004\n",
            "197: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0005, L-val: 0.5988\n",
            "198: Acc-tr: 100.00, Acc-val:  85.98, L-tr: 0.0005, L-val: 0.5957\n",
            "199: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0005, L-val: 0.5961\n",
            "200: Acc-tr: 100.00, Acc-val:  85.93, L-tr: 0.0005, L-val: 0.5978\n",
            "201: Acc-tr: 100.00, Acc-val:  85.94, L-tr: 0.0004, L-val: 0.5970\n",
            "202: Acc-tr: 100.00, Acc-val:  85.92, L-tr: 0.0004, L-val: 0.5968\n",
            "203: Acc-tr: 100.00, Acc-val:  85.94, L-tr: 0.0005, L-val: 0.5938\n",
            "204: Acc-tr: 100.00, Acc-val:  85.87, L-tr: 0.0005, L-val: 0.5983\n",
            "205: Acc-tr: 100.00, Acc-val:  85.98, L-tr: 0.0005, L-val: 0.5918\n",
            "206: Acc-tr: 100.00, Acc-val:  85.86, L-tr: 0.0005, L-val: 0.5941\n",
            "207: Acc-tr: 100.00, Acc-val:  86.10, L-tr: 0.0004, L-val: 0.5914\n",
            "208: Acc-tr: 100.00, Acc-val:  86.12, L-tr: 0.0004, L-val: 0.5936\n",
            "209: Acc-tr: 100.00, Acc-val:  85.90, L-tr: 0.0005, L-val: 0.5937\n",
            "210: Acc-tr: 100.00, Acc-val:  85.94, L-tr: 0.0004, L-val: 0.5936\n",
            "211: Acc-tr: 100.00, Acc-val:  85.99, L-tr: 0.0005, L-val: 0.5904\n",
            "212: Acc-tr: 100.00, Acc-val:  86.05, L-tr: 0.0004, L-val: 0.5906\n",
            "213: Acc-tr: 100.00, Acc-val:  86.20, L-tr: 0.0005, L-val: 0.5872\n",
            "214: Acc-tr: 100.00, Acc-val:  85.95, L-tr: 0.0004, L-val: 0.5888\n",
            "215: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0004, L-val: 0.5881\n",
            "216: Acc-tr: 100.00, Acc-val:  86.00, L-tr: 0.0004, L-val: 0.5893\n",
            "217: Acc-tr: 100.00, Acc-val:  85.92, L-tr: 0.0004, L-val: 0.5900\n",
            "218: Acc-tr: 100.00, Acc-val:  85.98, L-tr: 0.0005, L-val: 0.5840\n",
            "219: Acc-tr: 100.00, Acc-val:  85.95, L-tr: 0.0004, L-val: 0.5846\n",
            "220: Acc-tr: 100.00, Acc-val:  86.00, L-tr: 0.0004, L-val: 0.5868\n",
            "221: Acc-tr: 100.00, Acc-val:  86.07, L-tr: 0.0004, L-val: 0.5844\n",
            "222: Acc-tr: 100.00, Acc-val:  86.07, L-tr: 0.0004, L-val: 0.5860\n",
            "223: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0004, L-val: 0.5845\n",
            "224: Acc-tr: 100.00, Acc-val:  86.05, L-tr: 0.0004, L-val: 0.5821\n",
            "225: Acc-tr: 100.00, Acc-val:  86.05, L-tr: 0.0004, L-val: 0.5848\n",
            "226: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0004, L-val: 0.5827\n",
            "227: Acc-tr: 100.00, Acc-val:  86.02, L-tr: 0.0004, L-val: 0.5833\n",
            "228: Acc-tr: 100.00, Acc-val:  85.94, L-tr: 0.0005, L-val: 0.5827\n",
            "229: Acc-tr: 100.00, Acc-val:  85.90, L-tr: 0.0004, L-val: 0.5862\n",
            "230: Acc-tr: 100.00, Acc-val:  86.10, L-tr: 0.0004, L-val: 0.5846\n",
            "231: Acc-tr: 100.00, Acc-val:  86.02, L-tr: 0.0004, L-val: 0.5822\n",
            "232: Acc-tr: 100.00, Acc-val:  85.92, L-tr: 0.0004, L-val: 0.5840\n",
            "233: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0004, L-val: 0.5815\n",
            "234: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0004, L-val: 0.5820\n",
            "235: Acc-tr: 100.00, Acc-val:  86.04, L-tr: 0.0004, L-val: 0.5809\n",
            "236: Acc-tr: 100.00, Acc-val:  85.98, L-tr: 0.0004, L-val: 0.5876\n",
            "237: Acc-tr: 100.00, Acc-val:  86.02, L-tr: 0.0004, L-val: 0.5837\n",
            "238: Acc-tr: 100.00, Acc-val:  86.00, L-tr: 0.0004, L-val: 0.5827\n",
            "239: Acc-tr: 100.00, Acc-val:  86.06, L-tr: 0.0004, L-val: 0.5830\n",
            "240: Acc-tr: 100.00, Acc-val:  85.95, L-tr: 0.0005, L-val: 0.5843\n",
            "241: Acc-tr: 100.00, Acc-val:  85.96, L-tr: 0.0004, L-val: 0.5828\n",
            "242: Acc-tr: 100.00, Acc-val:  86.13, L-tr: 0.0005, L-val: 0.5803\n",
            "243: Acc-tr: 100.00, Acc-val:  86.14, L-tr: 0.0004, L-val: 0.5826\n",
            "244: Acc-tr: 100.00, Acc-val:  86.12, L-tr: 0.0004, L-val: 0.5838\n",
            "245: Acc-tr: 100.00, Acc-val:  86.08, L-tr: 0.0005, L-val: 0.5837\n",
            "246: Acc-tr: 100.00, Acc-val:  86.08, L-tr: 0.0005, L-val: 0.5847\n",
            "247: Acc-tr: 100.00, Acc-val:  86.04, L-tr: 0.0004, L-val: 0.5805\n",
            "248: Acc-tr: 100.00, Acc-val:  86.02, L-tr: 0.0005, L-val: 0.5822\n",
            "249: Acc-tr: 100.00, Acc-val:  86.05, L-tr: 0.0004, L-val: 0.5807\n",
            "250: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0004, L-val: 0.5810\n",
            "251: Acc-tr: 100.00, Acc-val:  86.12, L-tr: 0.0004, L-val: 0.5841\n",
            "252: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0004, L-val: 0.5811\n",
            "253: Acc-tr: 100.00, Acc-val:  86.04, L-tr: 0.0004, L-val: 0.5845\n",
            "254: Acc-tr: 100.00, Acc-val:  85.99, L-tr: 0.0004, L-val: 0.5834\n",
            "255: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0004, L-val: 0.5817\n",
            "256: Acc-tr: 100.00, Acc-val:  85.95, L-tr: 0.0004, L-val: 0.5829\n",
            "257: Acc-tr: 100.00, Acc-val:  85.96, L-tr: 0.0004, L-val: 0.5814\n",
            "258: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0004, L-val: 0.5830\n",
            "259: Acc-tr: 100.00, Acc-val:  85.99, L-tr: 0.0005, L-val: 0.5799\n",
            "260: Acc-tr: 100.00, Acc-val:  86.06, L-tr: 0.0004, L-val: 0.5819\n",
            "261: Acc-tr: 100.00, Acc-val:  86.09, L-tr: 0.0004, L-val: 0.5826\n",
            "262: Acc-tr: 100.00, Acc-val:  85.84, L-tr: 0.0005, L-val: 0.5805\n",
            "263: Acc-tr: 100.00, Acc-val:  86.07, L-tr: 0.0004, L-val: 0.5814\n",
            "264: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0005, L-val: 0.5789\n",
            "265: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0005, L-val: 0.5809\n",
            "266: Acc-tr: 100.00, Acc-val:  85.99, L-tr: 0.0005, L-val: 0.5804\n",
            "267: Acc-tr: 100.00, Acc-val:  85.99, L-tr: 0.0005, L-val: 0.5793\n",
            "268: Acc-tr: 100.00, Acc-val:  85.94, L-tr: 0.0004, L-val: 0.5863\n",
            "269: Acc-tr: 100.00, Acc-val:  86.12, L-tr: 0.0004, L-val: 0.5822\n",
            "270: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0004, L-val: 0.5830\n",
            "271: Acc-tr: 100.00, Acc-val:  86.05, L-tr: 0.0004, L-val: 0.5785\n",
            "272: Acc-tr: 100.00, Acc-val:  85.98, L-tr: 0.0004, L-val: 0.5828\n",
            "273: Acc-tr: 100.00, Acc-val:  86.09, L-tr: 0.0004, L-val: 0.5830\n",
            "274: Acc-tr: 100.00, Acc-val:  86.00, L-tr: 0.0004, L-val: 0.5797\n",
            "275: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0004, L-val: 0.5817\n",
            "276: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0004, L-val: 0.5843\n",
            "277: Acc-tr: 100.00, Acc-val:  86.00, L-tr: 0.0004, L-val: 0.5820\n",
            "278: Acc-tr: 100.00, Acc-val:  86.00, L-tr: 0.0004, L-val: 0.5797\n",
            "279: Acc-tr: 100.00, Acc-val:  86.00, L-tr: 0.0004, L-val: 0.5797\n",
            "280: Acc-tr: 100.00, Acc-val:  86.26, L-tr: 0.0005, L-val: 0.5805\n",
            "281: Acc-tr: 100.00, Acc-val:  85.94, L-tr: 0.0004, L-val: 0.5806\n",
            "282: Acc-tr: 100.00, Acc-val:  85.95, L-tr: 0.0004, L-val: 0.5820\n",
            "283: Acc-tr: 100.00, Acc-val:  86.05, L-tr: 0.0005, L-val: 0.5812\n",
            "284: Acc-tr: 100.00, Acc-val:  86.04, L-tr: 0.0005, L-val: 0.5772\n",
            "285: Acc-tr: 100.00, Acc-val:  85.95, L-tr: 0.0005, L-val: 0.5822\n",
            "286: Acc-tr: 100.00, Acc-val:  85.93, L-tr: 0.0005, L-val: 0.5803\n",
            "287: Acc-tr: 100.00, Acc-val:  86.09, L-tr: 0.0004, L-val: 0.5797\n",
            "288: Acc-tr: 100.00, Acc-val:  86.07, L-tr: 0.0004, L-val: 0.5806\n",
            "289: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0005, L-val: 0.5797\n",
            "290: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0004, L-val: 0.5815\n",
            "291: Acc-tr: 100.00, Acc-val:  85.92, L-tr: 0.0004, L-val: 0.5816\n",
            "292: Acc-tr: 100.00, Acc-val:  86.05, L-tr: 0.0004, L-val: 0.5830\n",
            "293: Acc-tr: 100.00, Acc-val:  86.02, L-tr: 0.0004, L-val: 0.5791\n",
            "294: Acc-tr: 100.00, Acc-val:  85.98, L-tr: 0.0005, L-val: 0.5794\n",
            "295: Acc-tr: 100.00, Acc-val:  85.97, L-tr: 0.0004, L-val: 0.5821\n",
            "296: Acc-tr: 100.00, Acc-val:  86.01, L-tr: 0.0004, L-val: 0.5802\n",
            "297: Acc-tr: 100.00, Acc-val:  85.93, L-tr: 0.0004, L-val: 0.5798\n",
            "298: Acc-tr: 100.00, Acc-val:  86.03, L-tr: 0.0005, L-val: 0.5768\n",
            "299: Acc-tr: 100.00, Acc-val:  86.11, L-tr: 0.0004, L-val: 0.5787\n",
            "<Figure size 640x480 with 2 Axes>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}